import os
import asyncio
from typing import List, Dict, Any, Optional
from abc import ABC, abstractmethod
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# å¼•å…¥ OpenAI å¼‚æ­¥å®¢æˆ·ç«¯
from openai import AsyncOpenAI

# å¼•å…¥ vLLM (ä»…åœ¨æœ¬åœ°æ¨¡å¼ä¸‹éœ€è¦ï¼Œä½¿ç”¨ try-import é˜²æ­¢ API æ¨¡å¼ä¸‹æŠ¥é”™)
try:
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False

class BaseModelInterface(ABC):
    """æ¨¡åž‹è°ƒç”¨æŠ½è±¡åŸºç±»"""
    @abstractmethod
    async def generate(self, messages: List[Dict], n: int = 1, temperature: float = 0.7, max_tokens: int = 1024) -> List[str]:
        pass

class APIInterface(BaseModelInterface):
    def __init__(self, model_name: str, api_key: str, base_url: Optional[str] = None):
        # ç¡®ä¿ api_key ä¸ä¸º Noneï¼Œå¦åˆ™ OpenAI åº“å¯èƒ½ä¼šæŠ¥é”™æˆ–å°è¯•è¯»å–çŽ¯å¢ƒå˜é‡
        if not api_key:
            api_key = "EMPTY"
        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        self.model_name = model_name

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(Exception)
    )
    async def generate(self, messages: List[Dict], n: int = 1, temperature: float = 0.7, max_tokens: int = 1024) -> List[str]:
        try:
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                n=n,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return [choice.message.content for choice in response.choices]
        except Exception as e:
            err_str = str(e).lower()
            
            # --- FIX START: ä¿®å¤è¯¯åˆ¤ "Unauthorized" ä¸º "n" å‚æ•°é”™è¯¯çš„é—®é¢˜ ---
            # åªæœ‰æ˜Žç¡®æ˜¯ BadRequestError ä¸”åŒ…å« parameter ç›¸å…³æè¿°æ‰é™çº§
            is_param_error = (
                "parameter" in err_str and 
                ("n" in err_str or "not supported" in err_str)
            )
            # æˆ–è€… DeepSeek ç‰¹æœ‰çš„é”™è¯¯ä¿¡æ¯
            if is_param_error:
                print(f"âš ï¸ API may not support n={n}, falling back to parallel requests...")
                return await self._generate_parallel(messages, n, temperature, max_tokens)
            
            # å¦‚æžœæ˜¯ 401 (Unauthorized) æˆ–å…¶ä»–é”™è¯¯ï¼Œç›´æŽ¥æŠ›å‡ºï¼Œä¸è¦é™çº§é‡è¯•
            if "unauthorized" in err_str or "401" in err_str:
                raise e
            # --- FIX END ---
            
            raise e

    async def _generate_parallel(self, messages, n, temperature, max_tokens):
        """é™çº§æ–¹æ¡ˆï¼šå¹¶å‘å‘é€ n ä¸ªè¯·æ±‚"""
        tasks = []
        for _ in range(n):
            tasks.append(self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                n=1,
                temperature=temperature,
                max_tokens=max_tokens
            ))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        outputs = []
        for res in results:
            if not isinstance(res, Exception):
                outputs.append(res.choices[0].message.content)
            else:
                print(f"âŒ One request failed: {res}")
                outputs.append("") # å¤±è´¥å ä½
        return outputs

class LocalVLLMInterface(BaseModelInterface):
    """
    é€‚ç”¨äºŽæœ¬åœ°æ˜¾å¡è¿è¡Œ vLLM (Offline Inference Mode)ã€‚
    æ³¨æ„ï¼švLLM é€šå¸¸æ˜¯åŒæ­¥é˜»å¡žçš„ï¼Œä¸”ç‹¬å  GPUã€‚ä¸è¦åœ¨ asyncio loop ä¸­ç›´æŽ¥æ··ç”¨ã€‚
    """
    def __init__(self, model_path: str, tensor_parallel_size: int = 1):
        if not VLLM_AVAILABLE:
            raise ImportError("vLLM not installed. Please pip install vllm.")
        
        print(f"ðŸš€ Loading vLLM model: {model_path}...")
        self.llm = LLM(model=model_path, tensor_parallel_size=tensor_parallel_size)

    async def generate(self, messages: List[Dict], n: int = 1, temperature: float = 0.7, max_tokens: int = 1024) -> List[str]:
        # vLLM çš„ç¦»çº¿æŽ¨ç†é€šå¸¸æŽ¥æ”¶ prompt string æˆ– tokens
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦ç®€å•çš„å°† messages è½¬æ¢ä¸º string (æˆ–è€…ä½¿ç”¨ tokenizer.apply_chat_template)
        # ä¸ºç®€åŒ–ï¼Œå‡è®¾ messages å·²ç»å¤„ç†å¥½ï¼Œæˆ–è€…æˆ‘ä»¬ç›´æŽ¥æ‹¼æŽ¥ prompt
        # è­¦å‘Šï¼švLLM çš„ Chat æ¨¡æ¿å¤„ç†æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå– user content
        # å®žé™…å¤çŽ°å»ºè®®ä½¿ç”¨ tokenizer çš„ chat template
        
        prompt_text = ""
        for msg in messages:
            prompt_text += f"{msg['role']}: {msg['content']}\n"
        prompt_text += "Assistant:"

        sampling_params = SamplingParams(
            n=n, 
            temperature=temperature, 
            max_tokens=max_tokens
        )
        
        # vLLM æ˜¯åŒæ­¥çš„ï¼Œä¸ºäº†é€‚é… async æŽ¥å£ï¼Œè¿™é‡Œå…¶å®žæ˜¯å‡å¼‚æ­¥
        # åœ¨æ‰¹é‡è„šæœ¬ä¸­ï¼Œæˆ‘ä»¬ä¼šæ”’ä¸€æ‰¹ prompt ä¸€èµ·å‘ç»™ vLLMï¼Œè€Œä¸æ˜¯å•æ¡è°ƒ
        outputs = self.llm.generate([prompt_text], sampling_params)
        return [output.text for output in outputs[0].outputs]

def get_model_interface(backend: str, **kwargs) -> BaseModelInterface:
    if backend == "api":
        return APIInterface(
            model_name=kwargs.get("model_name"),
            api_key=kwargs.get("api_key"),
            base_url=kwargs.get("base_url")
        )
    elif backend == "vllm":
        return LocalVLLMInterface(
            model_path=kwargs.get("model_name"),
            tensor_parallel_size=kwargs.get("tp", 1)
        )
    else:
        raise ValueError(f"Unknown backend: {backend}")