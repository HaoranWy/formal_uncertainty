import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss
from sklearn.calibration import calibration_curve

class Evaluator:
    def __init__(self):
        # å®šä¹‰æŒ‡æ ‡æ–¹å‘æ€§ï¼šascending=True è¡¨ç¤ºå€¼è¶Šå°è¶Šæ­£ç¡®(ä¸ç¡®å®šæ€§)ï¼ŒFalse è¡¨ç¤ºå€¼è¶Šå¤§è¶Šæ­£ç¡®(ç½®ä¿¡åº¦)
        self.metric_config = {
            'Grammar Entropy': {'ascending': True},
            'Perplexity': {'ascending': True},
            'KL Divergence': {'ascending': True},
            'NSUI': {'ascending': True},
            'Spectral Radius': {'ascending': True},
            'Rule Dist Kurtosis': {'ascending': True},
            'Self Consistency Text': {'ascending': False},
            'Self Consistency SMT': {'ascending': False},
            'Ensemble_Prob': {'ascending': False},
            'Ensemble ML': {'ascending': False},
            'Ensemble Average': {'ascending': False},
            'Ensemble Weighted': {'ascending': False},
            'Ensemble Simple': {'ascending': False}
        }

    def evaluate_metric(self, df, metric_name, target_col):
        """è®¡ç®—å•ä¸ªæŒ‡æ ‡çš„ç»Ÿè®¡æ•°æ®"""
        if metric_name not in df.columns: return None
        
        y_true = df[target_col].values
        raw_scores = df[metric_name].values
        
        # å»é™¤ NaN
        mask = ~np.isnan(raw_scores)
        y_true = y_true[mask]
        raw_scores = raw_scores[mask]
        
        if len(y_true) == 0: return None

        # ç»Ÿä¸€æ–¹å‘ï¼šå°†ä¸ç¡®å®šæ€§æŒ‡æ ‡å–åï¼Œå˜ä¸ºç½®ä¿¡åº¦æ–¹å‘
        config = self.metric_config.get(metric_name, {'ascending': True})
        if config['ascending']:
            y_score = -raw_scores
        else:
            y_score = raw_scores

        # è®¡ç®— AUROC
        try:
            auroc = roc_auc_score(y_true, y_score)
        except ValueError:
            auroc = 0.5

        # è®¡ç®— AURC (Area Under Risk-Coverage)
        # ç®€å•çš„é£é™©è¦†ç›–ç‡ç§¯åˆ†
        sorted_idx = np.argsort(y_score)[::-1] # ä»é«˜ç½®ä¿¡åº¦åˆ°ä½
        y_sorted = y_true[sorted_idx]
        n = len(y_sorted)
        # é£é™© = ç´¯ç§¯é”™è¯¯æ•° / å½“å‰è¦†ç›–æ ·æœ¬æ•°
        risk = np.cumsum(1 - y_sorted) / np.arange(1, n + 1)
        coverage = np.arange(1, n + 1) / n
        aurc = np.trapz(risk, coverage)

        # è®¡ç®— ECE (éœ€å½’ä¸€åŒ–åˆ° 0-1 æ¦‚ç‡ç©ºé—´)
        y_prob = (y_score - y_score.min()) / (y_score.max() - y_score.min() + 1e-9)
        ece = self._compute_ece(y_true, y_prob)

        return {
            "Metric": metric_name,
            "AUROC": auroc,
            "ECE": ece,
            "AURC": aurc
        }

    def _compute_ece(self, y_true, y_prob, n_bins=10):
        """è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— ECE"""
        bin_edges = np.linspace(0., 1., n_bins + 1)
        binids = np.digitize(y_prob, bin_edges) - 1
        
        ece = 0.0
        total = len(y_true)
        for i in range(n_bins):
            idx = binids == i
            if np.sum(idx) > 0:
                acc = np.mean(y_true[idx])
                conf = np.mean(y_prob[idx])
                weight = np.sum(idx) / total
                ece += weight * np.abs(acc - conf)
        return ece

    def plot_roc_curves(self, df, metrics, target_col, title="ROC Curves"):
        """
        ç»˜åˆ¶ ROC æ›²çº¿å¹¶ä¿å­˜åˆ°æœ¬åœ°
        """
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        save_dir = "outputs/plots"
        os.makedirs(save_dir, exist_ok=True)
        
        plt.figure(figsize=(10, 8))
        y_true = df[target_col].values
        
        # é¢œè‰²å¾ªç¯
        colors = plt.cm.get_cmap('tab10')
        
        for i, metric in enumerate(metrics):
            if metric not in df.columns: continue
            
            raw_scores = df[metric].values
            # å¤„ç† NaN
            mask = ~np.isnan(raw_scores)
            y_masked = y_true[mask]
            scores_masked = raw_scores[mask]
            
            if len(y_masked) < 2: continue

            # ç»Ÿä¸€æ–¹å‘
            config = self.metric_config.get(metric, {'ascending': True})
            if config['ascending']:
                scores_masked = -scores_masked
                
            fpr, tpr, _ = roc_curve(y_masked, scores_masked)
            auc = roc_auc_score(y_masked, scores_masked)
            
            plt.plot(fpr, tpr, lw=2, label=f'{metric} (AUC={auc:.4f})')

        # ç»˜åˆ¶å¯¹è§’çº¿
        plt.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(title)
        plt.legend(loc="lower right")
        plt.grid(alpha=0.3)
        
        # ä¿å­˜å›¾ç‰‡
        filename = f"{save_dir}/{title.replace(' ', '_').replace('/', '-')}.png"
        plt.savefig(filename, dpi=300)
        print(f"ğŸ–¼ï¸  ROC Plot saved to: {filename}")
        plt.close()