import os
import json
import argparse
import pandas as pd
from tqdm import tqdm
import warnings

# å¼•å…¥æˆ‘ä»¬ä¹‹å‰å†™å¥½çš„æ¨¡å—
from src.llm.prompts import PromptGenerator  # (Step 2.1)
from src.parsing.pcfg_builder import parse_and_count # (Step 3.1)
from src.parsing.pcfg_estimator import PCFGEstimator # (Step 3.2)
from src.metrics.calculator import PCFGMetricCalculator # (Step 3.3)
from src.evaluation.labeler import correctness_labeler # (Step 4.1)
from src.metrics.consistency import add_self_consistency_scores # (Step 4.x)
import time
# å¿½ç•¥ä¸€äº›ç§‘å­¦è®¡ç®—çš„ RuntimeWarning
warnings.filterwarnings('ignore')

class Pipeline:
    def __init__(self, input_file, output_file, dataset_name, sample_size=100):
        self.input_file = input_file
        self.output_file = output_file
        self.dataset_name = dataset_name
        self.sample_size = sample_size
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.labeler = correctness_labeler()
        
        # æ£€æŸ¥ç‚¹ç®¡ç†ï¼šè¯»å–å·²å¤„ç†çš„é—®é¢˜ID
        self.processed_ids = set()
        if os.path.exists(output_file):
            try:
                existing_df = pd.read_csv(output_file)
                if 'question_id' in existing_df.columns:
                    self.processed_ids = set(existing_df['question_id'].unique().astype(str))
                print(f"ğŸ”„ å‘ç°å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶ï¼Œå·²è·³è¿‡ {len(self.processed_ids)} ä¸ªé—®é¢˜ã€‚")
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥ï¼Œå°†è¦†ç›–æˆ–é‡æ–°å¼€å§‹: {e}")

    def load_data(self):
        """
        åŠ è½½è¾“å…¥æ•°æ® (JSONLæ ¼å¼)
        å‡è®¾æ¯è¡Œæ˜¯ä¸€ä¸ªé—®é¢˜ï¼ŒåŒ…å«å­—æ®µ: id, question, ground_truth
        ä»¥åŠé¢„ç”Ÿæˆçš„ samples: [{'smt_code':..., 'text_output':...}, ...]
        """
        data = []
        with open(self.input_file, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line)
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥åŒ¹é… processed_ids
                if str(item['id']) not in self.processed_ids:
                    data.append(item)
        return data

    def process_single_question(self, item):
        """
        æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼šå¤„ç†å•ä¸ªé—®é¢˜åŠå…¶ N ä¸ªæ ·æœ¬
        """
        q_id = str(item['id'])
        ground_truth = item['ground_truth']
        samples = item.get('samples', [])  # é¢„ç”Ÿæˆçš„ samples åˆ—è¡¨
        
        # å¦‚æœæ ·æœ¬ä¸å¤Ÿï¼Œè·³è¿‡ (æˆ–è€…åœ¨è¿™é‡Œè°ƒç”¨ LLM API å®æ—¶ç”Ÿæˆ)
        if not samples:
            return []

        # === Phase 1: Row-Level Processing (Z3 & Labeling) ===
        processed_rows = []
        valid_smt_codes_for_pcfg = [] # ç”¨äºæ„å»º PCFG çš„æœ‰æ•ˆä»£ç 
        
        for i, sample in enumerate(samples):
            smt_code = sample.get('smt_code', "")
            text_output = sample.get('text_output', "")
            
            # è°ƒç”¨ Labeler è¯„ä¼°æ­£ç¡®æ€§
            # æ„é€  labeler éœ€è¦çš„æ ¼å¼
            label_input = {
                "id": f"{q_id}_{i}",
                "ground_truth": ground_truth,
                "text_output": text_output,
                "smt_code": smt_code
            }
            
            res = self.labeler.process_sample(label_input)
            
            # ä¿å­˜è¯¥æ ·æœ¬çš„åŸºç¡€ä¿¡æ¯
            row = {
                "question_id": q_id,
                "sample_idx": i,
                "ground_truth": ground_truth,
                "smt_code": smt_code, # å¯é€‰ï¼šä¸ºäº† CSV ç˜¦èº«å¯ä»¥ä¸å­˜ä»£ç 
                "text_output": text_output,
                
                # Labeler ç»“æœ
                "smt_status": res["smt_executed_status"],
                "smt_pred_bool": res["smt_pred_bool"],
                "smt_is_correct": res["smt_is_correct"],
                "text_pred_bool": res["text_pred_bool"],
                "text_is_correct": res["text_is_correct"],
                "consistency_smt_text": res["consistency_smt_text"]
            }
            processed_rows.append(row)
            
            # æ”¶é›†ç”¨äº PCFG çš„ä»£ç  (ä»…æ”¶é›†éç©ºä»£ç ï¼ŒLabeler å†…éƒ¨ä¸åšè¯­æ³•æ£€æŸ¥ï¼ŒParser ä¼šåš)
            if smt_code and smt_code.strip():
                valid_smt_codes_for_pcfg.append(smt_code)

        # === Phase 2: Question-Level Processing (PCFG Construction) ===
        # ä½¿ç”¨ N ä¸ªæ ·æœ¬æ„å»º 1 ä¸ª PCFG
        # 1. Parse & Count
        rules_counter, valid_parse_count = parse_and_count(valid_smt_codes_for_pcfg)
        
        # 2. Estimate Probabilities (MLE + Laplace)
        estimator = PCFGEstimator(rules_counter, alpha=1.0)
        
        # 3. Calculate PCFG Metrics
        # æ³¨æ„ï¼šå¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½è§£æå¤±è´¥ï¼Œmetrics å°†ç”± Calculator è¿”å›é»˜è®¤é›¶å€¼
        pcfg_calc = PCFGMetricCalculator(estimator.pcfg_probs, start_symbol="script") # è§†ä½ çš„æ–‡æ³•è€Œå®š
        pcfg_metrics = pcfg_calc.compute_all()
        
        # æ·»åŠ è§£ææˆåŠŸç‡ä½œä¸ºé¢å¤–çš„ meta-feature
        pcfg_metrics['parse_success_rate'] = valid_parse_count / len(samples) if samples else 0
        
        # === Phase 3: Broadcasting & Merging ===
        # å°† PCFG æŒ‡æ ‡å¹¿æ’­ç»™è¯¥é—®é¢˜çš„æ¯ä¸€è¡Œ
        final_rows = []
        for row in processed_rows:
            # åˆå¹¶ä¸¤ä¸ªå­—å…¸
            merged_row = {**row, **pcfg_metrics}
            final_rows.append(merged_row)
            
        return final_rows

    def save_results(self, rows):
        """
        å¢é‡å†™å…¥ CSV
        """
        if not rows:
            return
            
        df = pd.DataFrame(rows)
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥ headerï¼›å¦‚æœå­˜åœ¨ï¼Œè¿½åŠ æ¨¡å¼ (mode='a') ä¸å†™ header
        need_header = not os.path.exists(self.output_file)
        
        df.to_csv(self.output_file, mode='a', header=need_header, index=False, encoding='utf-8')

    def run(self):
        """
        ä¸»æ‰§è¡Œå¾ªç¯
        """
        print(f"ğŸš€ Starting pipeline for dataset: {self.dataset_name}")
        data_to_process = self.load_data()
        print(f"ğŸ“ Loaded {len(data_to_process)} questions to process.")
        
        if len(data_to_process) == 0:
            print("ğŸ‰ No new data to process. All done!")
            return

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(data_to_process, desc="Processing Questions")
        
        batch_buffer = [] # å¯ä»¥åœ¨å†…å­˜é‡Œæ”’å‡ ä¸ªé—®é¢˜å†å†™ï¼Œè¿™é‡Œä¸ºäº†å®‰å…¨æ¯é¢˜å¿…å†™
        
        for item in pbar:
            try:
                # å¤„ç†å•ä¸ªé—®é¢˜
                rows = self.process_single_question(item)
                
                if rows:
                    # æš‚æ—¶è½¬æ¢æˆ DataFrame å¤„ç†ä¸€è‡´æ€§ (è™½ç„¶ Consistency éœ€è¦ GroupByï¼Œ
                    # ä½†è¿™é‡Œåªé’ˆå¯¹å•é¢˜ N ä¸ªæ ·æœ¬è®¡ç®— Consistency ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œ
                    # å› ä¸º GroupBy('question_id') åœ¨å•é¢˜æ•°æ®ä¸‹å°±æ˜¯å®ƒè‡ªå·±)
                    
                    # è®¡ç®— Self-Consistency (Step 4.x)
                    df_temp = pd.DataFrame(rows)
                    df_temp = add_self_consistency_scores(df_temp)
                    
                    # å­˜ç›˜
                    self.save_results(df_temp.to_dict('records'))
                    
            except Exception as e:
                error_id = item.get('id', 'unknown')
                print(f"\nâŒ Error processing question {error_id}: {str(e)}")
                # å¯ä»¥é€‰æ‹©è®°å½•é”™è¯¯æ—¥å¿—ï¼Œè€Œä¸æ˜¯ä¸­æ–­ç¨‹åº
                with open(f"output/logs/error_log_{time.time()}.txt", "a") as ef:
                    ef.write(f"{error_id}: {str(e)}\n")
                continue

        print(f"\nâœ… Pipeline finished. Results saved to {self.output_file}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Grammars of Uncertainty - Main Pipeline")
    parser.add_argument("--input", type=str, required=True, help="Input JSONL file with generations")
    parser.add_argument("--output", type=str, required=True, help="Output CSV file")
    parser.add_argument("--dataset", type=str, default="strategyqa", help="Dataset name")
    
    args = parser.parse_args()
    
    pipeline = Pipeline(
        input_file=args.input,
        output_file=args.output,
        dataset_name=args.dataset
    )
    
    pipeline.run()